# ğŸ§ Expert Linux - Chatbot RAG avec MCP & Ollama

Ce projet implÃ©mente un assistant intelligent spÃ©cialisÃ© dans l'administration systÃ¨me Linux. Il repose sur une architecture moderne utilisant le protocole **MCP (Model Context Protocol)** pour connecter un modÃ¨le d'IA local Ã  une base de connaissances personnalisÃ©e (RAG).

[Image of a RAG architecture showing the flow between a user query, an MCP server, a vector database, and a local LLM like Ollama]

## ğŸš€ FonctionnalitÃ©s
- **RAG Local** : Interroge une base de donnÃ©es de commandes Linux sans envoyer de donnÃ©es dans le cloud.
- **Protocole MCP** : Utilise `FastMCP` pour exposer les outils de recherche via un serveur SSE.
- **Streaming** : Affichage de la rÃ©ponse en temps rÃ©el (mot par mot) dans le terminal.
- **Agentic Design** : Comportement de l'expert dÃ©fini via un fichier `linux_expert.md` modulaire.

---

## ğŸ—ï¸ Architecture du Projet

```text
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                # Point d'entrÃ©e (Client Agent)
â”‚   â”œâ”€â”€ linux_expert.md        # IdentitÃ© et rÃ¨gles de l'agent
â”‚   â”œâ”€â”€ chat_engine/
â”‚   â”‚   â””â”€â”€ generate_answer.py # Logique de streaming et appels RAG
â”‚   â”œâ”€â”€ mcp_server/
â”‚   â”‚   â””â”€â”€ server.py          # Serveur MCP (FastAPI + SSE)
â”‚   â””â”€â”€ rag_engine/
â”‚       â”œâ”€â”€ ingest.py          # Script d'indexation des donnÃ©es
â”‚       â””â”€â”€ retriever.py       # Logique de recherche documentaire
â”œâ”€â”€ data/
â”‚   â””â”€â”€ linux_commands.json    # Base de donnÃ©es brute
â”œâ”€â”€ requirements.txt           # DÃ©pendances Python
â””â”€â”€ README.md